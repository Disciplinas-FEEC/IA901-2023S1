{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMfPxPNXXVmvYCxQ8vOVDsQ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Objetivo**: Treinar uma rede neural convolucional para identificar o tipo de tecido\n"],"metadata":{"id":"JlwiZ52dqhtO"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","PATH = '/content/gdrive/Shareddrives/IA901 - Projeto Final/'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"edeH_tAO2QSS","outputId":"1fda52c1-fa9f-4f00-afab-8e5c77319dfc","executionInfo":{"status":"ok","timestamp":1687259615042,"user_tz":180,"elapsed":91647,"user":{"displayName":"Gianni Shigeru Setoue Liveraro","userId":"06291562626291357927"}}},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"ExecuteTime":{"end_time":"2019-10-08T07:29:06.996023Z","start_time":"2019-10-08T07:29:05.475794Z"},"colab":{"base_uri":"https://localhost:8080/"},"id":"tU3bdMfArp7Y","outputId":"bbd1cff4-233e-4b0e-9d0b-c6b73784516c","executionInfo":{"status":"ok","timestamp":1687259617411,"user_tz":180,"elapsed":2391,"user":{"displayName":"Gianni Shigeru Setoue Liveraro","userId":"06291562626291357927"}}},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<contextlib.ExitStack at 0x7f893162beb0>"]},"metadata":{},"execution_count":2}],"source":["# Imports\n","\n","# import the needed libs\n","\n","from __future__ import print_function, division\n","\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.optim import lr_scheduler\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, models, transforms\n","from torchsummary import summary\n","import matplotlib.pyplot as plt\n","import time\n","import os\n","import copy\n","\n","torch.manual_seed(42) # semente aleatoria!!!\n","\n","plt.ion()   # interactive mode"]},{"cell_type":"code","source":["def MakeREADME(file_path, text):\n","  path = file_path+\"/README.txt\"  # File path and name\n","  # Open the file in write mode\n","  file = open(path, \"w\")\n","  # Write the text to the file\n","  file.write(text)\n","  # Close the file\n","  file.close()"],"metadata":{"id":"gLuCdRoCa4lx","executionInfo":{"status":"ok","timestamp":1687259617413,"user_tz":180,"elapsed":14,"user":{"displayName":"Gianni Shigeru Setoue Liveraro","userId":"06291562626291357927"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# ______________________________________________________________________________\n","# Cria pasta para salvar resultados do experimento como um todo\n","import os\n","\n","ExperimentName = 'Experiment_II'\n","\n","path_interim = PATH+\"Notebooks/InterimResults/TissueClassification/{}\".format(ExperimentName)\n","if not os.path.exists(path_interim):\n","  print('Criar pasta chamada {}? (yes/no)'.format(ExperimentName))\n","  Answer2 = str (input())\n","\n","  if Answer2=='yes':\n","    os.mkdir(path_interim)\n","    print('Resuma o conteudo deste grande experimento. Qual a sua principal diferença para os demais?')\n","    text = str (input())\n","    MakeREADME(path_interim, text)\n","\n","# ______________________________________________________________________________\n","# Cria subdiretorio para salvar resultados dos subexperimentos\n","print('Quer criar um novo sub_experimento? (True/False)')\n","NewSubExperiment = str (input())\n","\n","if NewSubExperiment=='True':\n","\n","  Experiment_Number = len(next(os.walk(path_interim))[1])\n","  subpath = path_interim+'/{}'.format(Experiment_Number+1)\n","  os.mkdir(subpath)\n","\n","  print('Resuma o conteudo deste sub experimento. Qual a sua principal mudança feita')\n","  text = str (input())\n","  MakeREADME(subpath, text)\n","\n","else:\n","\n","  print('Diga o numero do experimento que você gostaria de refazer (Experiment_Number)')\n","  Experiment_Number = str (input())\n","  subpath = path_interim+'/{}'.format(Experiment_Number)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9iyNmV3UPxRl","executionInfo":{"status":"ok","timestamp":1687259708108,"user_tz":180,"elapsed":90706,"user":{"displayName":"Gianni Shigeru Setoue Liveraro","userId":"06291562626291357927"}},"outputId":"bc49f406-4744-447e-cada-85896d826a97"},"execution_count":4,"outputs":[{"name":"stdout","output_type":"stream","text":["Criar pasta chamada Experiment_II? (yes/no)\n","yes\n","Resuma o conteudo deste grande experimento. Qual a sua principal diferença para os demais?\n","Neste caso, usaremos Colon como a classe positiva e os demais tecidos como classe negativa. Empregaremos a EfficientNetb0 pra isso.\n","Quer criar um novo sub_experimento? (True/False)\n","True\n","Resuma o conteudo deste sub experimento. Qual a sua principal mudança feita\n","Primeiro run usando as mesmas configurações do Experimento I\n"]}]},{"cell_type":"markdown","source":["# Configurações gerais e funções úteis para as análises"],"metadata":{"id":"sySCAGAVrmPk"}},{"cell_type":"code","source":["from sklearn import metrics\n","from matplotlib import pyplot as plt\n","import pandas as pd\n","\n","def AnalysisPlots(classe_teste, PrevisoesProb):\n","\n","  # Construindo a ROC Curve\n","  # A função abaixo retorna arrays de True Positive Rate (TPR), False Positive Rate (FPR) e os valores de threshold\n","  FPR, TPR, thresholds = metrics.roc_curve(classe_teste, PrevisoesProb, drop_intermediate=False)\n","\n","  # Calculo da AUC\n","  auc = metrics.roc_auc_score(classe_teste, PrevisoesProb)\n","\n","  # Plotando ROC Curve com matplotlib:\n","  fig = plt.figure()\n","  ax = fig.add_subplot(111)\n","\n","  ax.plot(FPR, TPR, label=\"AUC =\"+np.format_float_positional(auc, precision=3))\n","  ax.plot([0,1], [0,1], linestyle='--', label='Random Selection, AUC = 0.5')\n","  ax.set_xlabel('False Positive Rate', fontsize=15)\n","  ax.set_ylabel('True Positive Rate', fontsize=15)\n","  ax.set_title('ROC Curve', fontsize=15)\n","  ax.grid()\n","  ax.legend(loc='lower right')\n","  plt.savefig(subpath+'/ROCCurve.png')\n","  #plt.show()\n","\n","  plt.clf() # limpando janela de plot\n","\n","  # Plot TPR x Threshold\n","\n","  fig = plt.figure()\n","  ax = fig.add_subplot(111)\n","\n","  ax.plot(thresholds, TPR)\n","  ax.set_xlabel('Decision Threshold', fontsize=15)\n","  ax.set_ylabel('True Positive Rate', fontsize=15)\n","  ax.set_title('TPR x Decision Threshold', fontsize=15)\n","  ax.grid()\n","  ax.set_xlim((0,1.0))\n","  plt.savefig(subpath+'/TPRxThresholf.png')\n","  #plt.show()\n","\n","  plt.clf() # limpando janela de plot\n","\n","  # Plot FPR x Threshold\n","\n","  fig = plt.figure()\n","  ax = fig.add_subplot(111)\n","\n","  ax.plot(thresholds, FPR)\n","  ax.set_xlabel('Decision Threshold', fontsize=15)\n","  ax.set_ylabel('False Positive Rate', fontsize=15)\n","  ax.set_title('FPR x Decision Threshold', fontsize=15)\n","  ax.grid()\n","  ax.set_xlim((0,1.0))\n","  #plt.show()\n","\n","  # Comando para salvar figura:\n","  plt.savefig(subpath+'/FPRxThresholf.png')\n","\n","  plt.clf() # limpando janela de plot\n","\n","  # Dataframe com indice de Youden:\n","  DataFrameYouden = pd.DataFrame(data={'Threshold':thresholds,'Youden':TPR - FPR})\n","\n","  # Dataframe com indice maximo de Youden:\n","  DataFrameYoudenMax = DataFrameYouden.sort_values(by='Youden', ascending = False) # Ordenando em ordem crescente\n","\n","  # Print do valor de threshold que maximiza o índice de Youden:\n","  print('\\n Melhor Decision Threshold: ', DataFrameYoudenMax.Threshold.values[0]) # printando valor de threshold que maximiza o Youden\n","  print('\\n Melhor Índice de Youden: ', DataFrameYoudenMax.Youden.values[0]) # printando valor maximo do Youden\n","\n","\n","def Variable_x_Epochs(Epochs, Variable1, Variable2, metricname):\n","  fig = plt.figure()\n","  ax = fig.add_subplot(111)\n","\n","  ax.plot(Epochs, Variable1, label='Train')\n","  ax.plot(Epochs, Variable2, label='Val')\n","  ax.set_xlabel('Epochs', fontsize=15)\n","  ax.set_ylabel('{}'.format(metricname), fontsize=15)\n","  plt.legend()\n","  ax.grid()\n","  plt.savefig(subpath+'/Epochsx{}.png'.format(metricname))\n","  plt.show()"],"metadata":{"id":"TyPOHKkc_cji","executionInfo":{"status":"ok","timestamp":1687259708920,"user_tz":180,"elapsed":825,"user":{"displayName":"Gianni Shigeru Setoue Liveraro","userId":"06291562626291357927"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"553If69Irp7Z"},"source":["## Carregar dados\n"]},{"cell_type":"code","source":["class ImageFolderWithPath(datasets.ImageFolder):\n","    def __getitem__(self, index):\n","        path, target = self.samples[index]\n","        img = self.loader(path)\n","        if self.transform is not None:\n","            img = self.transform(img)\n","        if self.target_transform is not None:\n","            target = self.target_transform(target)\n","        return img, target, path"],"metadata":{"id":"UwLEF_2j1C3E","executionInfo":{"status":"ok","timestamp":1687259708922,"user_tz":180,"elapsed":12,"user":{"displayName":"Gianni Shigeru Setoue Liveraro","userId":"06291562626291357927"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ef73p3_Drp7c"},"source":["## Treinando o modelo"]},{"cell_type":"code","execution_count":7,"metadata":{"ExecuteTime":{"end_time":"2019-10-08T07:29:32.281807Z","start_time":"2019-10-08T07:29:32.270836Z"},"code_folding":[],"id":"DbZlV84Hrp7c","executionInfo":{"status":"ok","timestamp":1687259708924,"user_tz":180,"elapsed":12,"user":{"displayName":"Gianni Shigeru Setoue Liveraro","userId":"06291562626291357927"}}},"outputs":[],"source":["def train_model(model, criterion, optimizer, scheduler=None, num_epochs=25):\n","    since = time.time()\n","\n","    LOSS_train = []\n","    LOSS_val = []\n","    ACC_train = []\n","    ACC_val = []\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_loss = 1.0\n","    best_epoch = 0\n","    best_acc = 0\n","    best_auc = 0\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","            Probabilities = []\n","            ClassLabels = []\n","\n","            BatchTotalSize = len(dataloaders[phase])\n","\n","            batch_counter = 0\n","            # Iterate over data.\n","            for inputs, labels, _ in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        batch_counter = batch_counter + 1\n","                        print('\\r Steps: {} out {}'.format(batch_counter, BatchTotalSize), end=\" \")\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                Probabilities.append(outputs.cpu().detach().numpy()[:,1])\n","                ClassLabels.append(labels.cpu().detach().numpy())\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","\n","            ClassLabels = np.concatenate(ClassLabels).ravel()\n","            Probabilities = np.concatenate(Probabilities).ravel()\n","\n","            #print('labels', ClassLabels)\n","            #print('probs', Probabilities)\n","\n","            if phase == 'train' and scheduler!=None:\n","                scheduler.step()\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","            epoch_auc = metrics.roc_auc_score(ClassLabels, Probabilities)\n","\n","            print('{} Loss: {:.4f} Acc: {:.4f} AUC: {:.4f}'.format(\n","                phase, epoch_loss, epoch_acc, epoch_auc))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_loss < best_loss: # criterio de minimizacao do loss\n","                print('Best model detected!')\n","                best_loss = epoch_loss\n","                best_auc = epoch_auc\n","                best_acc = epoch_acc\n","                best_epoch = epoch\n","\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","\n","            if phase == 'train':\n","                LOSS_train.append(epoch_loss)\n","                ACC_train.append(epoch_acc.cpu().detach().numpy())\n","            if phase == 'val':\n","                LOSS_val.append(epoch_loss)\n","                ACC_val.append(epoch_acc.cpu().detach().numpy())\n","\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best model (epoch={}): val Loss: {:5f} Acc: {:5f} val AUC: {:5f}'.format(best_epoch, best_loss, best_acc, best_auc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","\n","    return model, LOSS_train, ACC_train, LOSS_val, ACC_val\n"]},{"cell_type":"markdown","source":["## Testando o modelo"],"metadata":{"id":"yMkKMGwhyAJs"}},{"cell_type":"code","source":["def validate_model(model):\n","    model.eval()\n","\n","    Probabilities = []\n","    ClassLabels = []\n","\n","    with torch.no_grad():\n","\n","            # Iterate over data.\n","            for inputs, labels, _ in dataloaders['val']:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                outputs = model(inputs)\n","\n","                Probabilities.append(outputs.cpu().detach().numpy()[:,1])\n","                ClassLabels.append(labels.cpu().detach().numpy())\n","\n","    return np.concatenate(Probabilities).ravel(), np.concatenate(ClassLabels).ravel()"],"metadata":{"id":"54phGbTLx_Kg","executionInfo":{"status":"ok","timestamp":1687259708926,"user_tz":180,"elapsed":13,"user":{"displayName":"Gianni Shigeru Setoue Liveraro","userId":"06291562626291357927"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["# Experimento:\n","\n"],"metadata":{"id":"QJcUlNaxVEnQ"}},{"cell_type":"code","source":["# Data augmentation and normalization for training\n","\n","data_dir = PATH+'Datasets/Processed/TissueClassification/{}'.format(ExperimentName)\n","\n","size, padding = 224, 60 # tamanho de entrada da EfficientNet B1\n","\n","data_transforms = {\n","    'train': transforms.Compose([\n","        transforms.Resize((size,size)),\n","        transforms.RandomHorizontalFlip(p=0.5),\n","        transforms.RandomVerticalFlip(p=0.5),\n","        #transforms.Pad(padding),\n","        transforms.RandomCrop(size, padding,padding_mode='reflect'),\n","        transforms.ToTensor(),\n","    ]),\n","    'val': transforms.Compose([\n","        transforms.Resize((size,size)),\n","        transforms.ToTensor()\n","    ])\n","}\n","\n","image_datasets = {x: ImageFolderWithPath(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n","\n","dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=16, shuffle=True, num_workers=2) for x in ['train', 'val']}\n","\n","dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n","\n","class_names = image_datasets['train'].classes\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","print(dataset_sizes)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687262282250,"user_tz":180,"elapsed":620,"user":{"displayName":"Gianni Shigeru Setoue Liveraro","userId":"06291562626291357927"}},"outputId":"6a80703e-210f-4e44-81c6-58e8828472f6","id":"s0WQmuneuIuf"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["{'train': 5539, 'val': 778}\n"]}]},{"cell_type":"code","source":["model = models.efficientnet_b0(weights=True)\n","model.classifier[1] = nn.Linear(in_features= model.classifier[1].in_features, out_features=2)\n","\n","# Add a softmax activation to the output layer\n","model = nn.Sequential(model, nn.Softmax(dim=1))\n","\n","model = model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","criterion = criterion.to(device)\n","\n","num_epochs = 100 # epochs\n","lr=1e-4 # learning rate\n","\n","# Observe that all parameters are being optimized\n","optimizer_model = optim.Adam(model.parameters(), lr=lr)\n","\n","summary(model, (3, size, size))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1687262288101,"user_tz":180,"elapsed":531,"user":{"displayName":"Gianni Shigeru Setoue Liveraro","userId":"06291562626291357927"}},"outputId":"29ff43f8-32cf-4d2d-ca03-c3f7491c824f","id":"F5c79MTpuIu5"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 32, 112, 112]             864\n","       BatchNorm2d-2         [-1, 32, 112, 112]              64\n","              SiLU-3         [-1, 32, 112, 112]               0\n","            Conv2d-4         [-1, 32, 112, 112]             288\n","       BatchNorm2d-5         [-1, 32, 112, 112]              64\n","              SiLU-6         [-1, 32, 112, 112]               0\n"," AdaptiveAvgPool2d-7             [-1, 32, 1, 1]               0\n","            Conv2d-8              [-1, 8, 1, 1]             264\n","              SiLU-9              [-1, 8, 1, 1]               0\n","           Conv2d-10             [-1, 32, 1, 1]             288\n","          Sigmoid-11             [-1, 32, 1, 1]               0\n","SqueezeExcitation-12         [-1, 32, 112, 112]               0\n","           Conv2d-13         [-1, 16, 112, 112]             512\n","      BatchNorm2d-14         [-1, 16, 112, 112]              32\n","           MBConv-15         [-1, 16, 112, 112]               0\n","           Conv2d-16         [-1, 96, 112, 112]           1,536\n","      BatchNorm2d-17         [-1, 96, 112, 112]             192\n","             SiLU-18         [-1, 96, 112, 112]               0\n","           Conv2d-19           [-1, 96, 56, 56]             864\n","      BatchNorm2d-20           [-1, 96, 56, 56]             192\n","             SiLU-21           [-1, 96, 56, 56]               0\n","AdaptiveAvgPool2d-22             [-1, 96, 1, 1]               0\n","           Conv2d-23              [-1, 4, 1, 1]             388\n","             SiLU-24              [-1, 4, 1, 1]               0\n","           Conv2d-25             [-1, 96, 1, 1]             480\n","          Sigmoid-26             [-1, 96, 1, 1]               0\n","SqueezeExcitation-27           [-1, 96, 56, 56]               0\n","           Conv2d-28           [-1, 24, 56, 56]           2,304\n","      BatchNorm2d-29           [-1, 24, 56, 56]              48\n","           MBConv-30           [-1, 24, 56, 56]               0\n","           Conv2d-31          [-1, 144, 56, 56]           3,456\n","      BatchNorm2d-32          [-1, 144, 56, 56]             288\n","             SiLU-33          [-1, 144, 56, 56]               0\n","           Conv2d-34          [-1, 144, 56, 56]           1,296\n","      BatchNorm2d-35          [-1, 144, 56, 56]             288\n","             SiLU-36          [-1, 144, 56, 56]               0\n","AdaptiveAvgPool2d-37            [-1, 144, 1, 1]               0\n","           Conv2d-38              [-1, 6, 1, 1]             870\n","             SiLU-39              [-1, 6, 1, 1]               0\n","           Conv2d-40            [-1, 144, 1, 1]           1,008\n","          Sigmoid-41            [-1, 144, 1, 1]               0\n","SqueezeExcitation-42          [-1, 144, 56, 56]               0\n","           Conv2d-43           [-1, 24, 56, 56]           3,456\n","      BatchNorm2d-44           [-1, 24, 56, 56]              48\n","  StochasticDepth-45           [-1, 24, 56, 56]               0\n","           MBConv-46           [-1, 24, 56, 56]               0\n","           Conv2d-47          [-1, 144, 56, 56]           3,456\n","      BatchNorm2d-48          [-1, 144, 56, 56]             288\n","             SiLU-49          [-1, 144, 56, 56]               0\n","           Conv2d-50          [-1, 144, 28, 28]           3,600\n","      BatchNorm2d-51          [-1, 144, 28, 28]             288\n","             SiLU-52          [-1, 144, 28, 28]               0\n","AdaptiveAvgPool2d-53            [-1, 144, 1, 1]               0\n","           Conv2d-54              [-1, 6, 1, 1]             870\n","             SiLU-55              [-1, 6, 1, 1]               0\n","           Conv2d-56            [-1, 144, 1, 1]           1,008\n","          Sigmoid-57            [-1, 144, 1, 1]               0\n","SqueezeExcitation-58          [-1, 144, 28, 28]               0\n","           Conv2d-59           [-1, 40, 28, 28]           5,760\n","      BatchNorm2d-60           [-1, 40, 28, 28]              80\n","           MBConv-61           [-1, 40, 28, 28]               0\n","           Conv2d-62          [-1, 240, 28, 28]           9,600\n","      BatchNorm2d-63          [-1, 240, 28, 28]             480\n","             SiLU-64          [-1, 240, 28, 28]               0\n","           Conv2d-65          [-1, 240, 28, 28]           6,000\n","      BatchNorm2d-66          [-1, 240, 28, 28]             480\n","             SiLU-67          [-1, 240, 28, 28]               0\n","AdaptiveAvgPool2d-68            [-1, 240, 1, 1]               0\n","           Conv2d-69             [-1, 10, 1, 1]           2,410\n","             SiLU-70             [-1, 10, 1, 1]               0\n","           Conv2d-71            [-1, 240, 1, 1]           2,640\n","          Sigmoid-72            [-1, 240, 1, 1]               0\n","SqueezeExcitation-73          [-1, 240, 28, 28]               0\n","           Conv2d-74           [-1, 40, 28, 28]           9,600\n","      BatchNorm2d-75           [-1, 40, 28, 28]              80\n","  StochasticDepth-76           [-1, 40, 28, 28]               0\n","           MBConv-77           [-1, 40, 28, 28]               0\n","           Conv2d-78          [-1, 240, 28, 28]           9,600\n","      BatchNorm2d-79          [-1, 240, 28, 28]             480\n","             SiLU-80          [-1, 240, 28, 28]               0\n","           Conv2d-81          [-1, 240, 14, 14]           2,160\n","      BatchNorm2d-82          [-1, 240, 14, 14]             480\n","             SiLU-83          [-1, 240, 14, 14]               0\n","AdaptiveAvgPool2d-84            [-1, 240, 1, 1]               0\n","           Conv2d-85             [-1, 10, 1, 1]           2,410\n","             SiLU-86             [-1, 10, 1, 1]               0\n","           Conv2d-87            [-1, 240, 1, 1]           2,640\n","          Sigmoid-88            [-1, 240, 1, 1]               0\n","SqueezeExcitation-89          [-1, 240, 14, 14]               0\n","           Conv2d-90           [-1, 80, 14, 14]          19,200\n","      BatchNorm2d-91           [-1, 80, 14, 14]             160\n","           MBConv-92           [-1, 80, 14, 14]               0\n","           Conv2d-93          [-1, 480, 14, 14]          38,400\n","      BatchNorm2d-94          [-1, 480, 14, 14]             960\n","             SiLU-95          [-1, 480, 14, 14]               0\n","           Conv2d-96          [-1, 480, 14, 14]           4,320\n","      BatchNorm2d-97          [-1, 480, 14, 14]             960\n","             SiLU-98          [-1, 480, 14, 14]               0\n","AdaptiveAvgPool2d-99            [-1, 480, 1, 1]               0\n","          Conv2d-100             [-1, 20, 1, 1]           9,620\n","            SiLU-101             [-1, 20, 1, 1]               0\n","          Conv2d-102            [-1, 480, 1, 1]          10,080\n","         Sigmoid-103            [-1, 480, 1, 1]               0\n","SqueezeExcitation-104          [-1, 480, 14, 14]               0\n","          Conv2d-105           [-1, 80, 14, 14]          38,400\n","     BatchNorm2d-106           [-1, 80, 14, 14]             160\n"," StochasticDepth-107           [-1, 80, 14, 14]               0\n","          MBConv-108           [-1, 80, 14, 14]               0\n","          Conv2d-109          [-1, 480, 14, 14]          38,400\n","     BatchNorm2d-110          [-1, 480, 14, 14]             960\n","            SiLU-111          [-1, 480, 14, 14]               0\n","          Conv2d-112          [-1, 480, 14, 14]           4,320\n","     BatchNorm2d-113          [-1, 480, 14, 14]             960\n","            SiLU-114          [-1, 480, 14, 14]               0\n","AdaptiveAvgPool2d-115            [-1, 480, 1, 1]               0\n","          Conv2d-116             [-1, 20, 1, 1]           9,620\n","            SiLU-117             [-1, 20, 1, 1]               0\n","          Conv2d-118            [-1, 480, 1, 1]          10,080\n","         Sigmoid-119            [-1, 480, 1, 1]               0\n","SqueezeExcitation-120          [-1, 480, 14, 14]               0\n","          Conv2d-121           [-1, 80, 14, 14]          38,400\n","     BatchNorm2d-122           [-1, 80, 14, 14]             160\n"," StochasticDepth-123           [-1, 80, 14, 14]               0\n","          MBConv-124           [-1, 80, 14, 14]               0\n","          Conv2d-125          [-1, 480, 14, 14]          38,400\n","     BatchNorm2d-126          [-1, 480, 14, 14]             960\n","            SiLU-127          [-1, 480, 14, 14]               0\n","          Conv2d-128          [-1, 480, 14, 14]          12,000\n","     BatchNorm2d-129          [-1, 480, 14, 14]             960\n","            SiLU-130          [-1, 480, 14, 14]               0\n","AdaptiveAvgPool2d-131            [-1, 480, 1, 1]               0\n","          Conv2d-132             [-1, 20, 1, 1]           9,620\n","            SiLU-133             [-1, 20, 1, 1]               0\n","          Conv2d-134            [-1, 480, 1, 1]          10,080\n","         Sigmoid-135            [-1, 480, 1, 1]               0\n","SqueezeExcitation-136          [-1, 480, 14, 14]               0\n","          Conv2d-137          [-1, 112, 14, 14]          53,760\n","     BatchNorm2d-138          [-1, 112, 14, 14]             224\n","          MBConv-139          [-1, 112, 14, 14]               0\n","          Conv2d-140          [-1, 672, 14, 14]          75,264\n","     BatchNorm2d-141          [-1, 672, 14, 14]           1,344\n","            SiLU-142          [-1, 672, 14, 14]               0\n","          Conv2d-143          [-1, 672, 14, 14]          16,800\n","     BatchNorm2d-144          [-1, 672, 14, 14]           1,344\n","            SiLU-145          [-1, 672, 14, 14]               0\n","AdaptiveAvgPool2d-146            [-1, 672, 1, 1]               0\n","          Conv2d-147             [-1, 28, 1, 1]          18,844\n","            SiLU-148             [-1, 28, 1, 1]               0\n","          Conv2d-149            [-1, 672, 1, 1]          19,488\n","         Sigmoid-150            [-1, 672, 1, 1]               0\n","SqueezeExcitation-151          [-1, 672, 14, 14]               0\n","          Conv2d-152          [-1, 112, 14, 14]          75,264\n","     BatchNorm2d-153          [-1, 112, 14, 14]             224\n"," StochasticDepth-154          [-1, 112, 14, 14]               0\n","          MBConv-155          [-1, 112, 14, 14]               0\n","          Conv2d-156          [-1, 672, 14, 14]          75,264\n","     BatchNorm2d-157          [-1, 672, 14, 14]           1,344\n","            SiLU-158          [-1, 672, 14, 14]               0\n","          Conv2d-159          [-1, 672, 14, 14]          16,800\n","     BatchNorm2d-160          [-1, 672, 14, 14]           1,344\n","            SiLU-161          [-1, 672, 14, 14]               0\n","AdaptiveAvgPool2d-162            [-1, 672, 1, 1]               0\n","          Conv2d-163             [-1, 28, 1, 1]          18,844\n","            SiLU-164             [-1, 28, 1, 1]               0\n","          Conv2d-165            [-1, 672, 1, 1]          19,488\n","         Sigmoid-166            [-1, 672, 1, 1]               0\n","SqueezeExcitation-167          [-1, 672, 14, 14]               0\n","          Conv2d-168          [-1, 112, 14, 14]          75,264\n","     BatchNorm2d-169          [-1, 112, 14, 14]             224\n"," StochasticDepth-170          [-1, 112, 14, 14]               0\n","          MBConv-171          [-1, 112, 14, 14]               0\n","          Conv2d-172          [-1, 672, 14, 14]          75,264\n","     BatchNorm2d-173          [-1, 672, 14, 14]           1,344\n","            SiLU-174          [-1, 672, 14, 14]               0\n","          Conv2d-175            [-1, 672, 7, 7]          16,800\n","     BatchNorm2d-176            [-1, 672, 7, 7]           1,344\n","            SiLU-177            [-1, 672, 7, 7]               0\n","AdaptiveAvgPool2d-178            [-1, 672, 1, 1]               0\n","          Conv2d-179             [-1, 28, 1, 1]          18,844\n","            SiLU-180             [-1, 28, 1, 1]               0\n","          Conv2d-181            [-1, 672, 1, 1]          19,488\n","         Sigmoid-182            [-1, 672, 1, 1]               0\n","SqueezeExcitation-183            [-1, 672, 7, 7]               0\n","          Conv2d-184            [-1, 192, 7, 7]         129,024\n","     BatchNorm2d-185            [-1, 192, 7, 7]             384\n","          MBConv-186            [-1, 192, 7, 7]               0\n","          Conv2d-187           [-1, 1152, 7, 7]         221,184\n","     BatchNorm2d-188           [-1, 1152, 7, 7]           2,304\n","            SiLU-189           [-1, 1152, 7, 7]               0\n","          Conv2d-190           [-1, 1152, 7, 7]          28,800\n","     BatchNorm2d-191           [-1, 1152, 7, 7]           2,304\n","            SiLU-192           [-1, 1152, 7, 7]               0\n","AdaptiveAvgPool2d-193           [-1, 1152, 1, 1]               0\n","          Conv2d-194             [-1, 48, 1, 1]          55,344\n","            SiLU-195             [-1, 48, 1, 1]               0\n","          Conv2d-196           [-1, 1152, 1, 1]          56,448\n","         Sigmoid-197           [-1, 1152, 1, 1]               0\n","SqueezeExcitation-198           [-1, 1152, 7, 7]               0\n","          Conv2d-199            [-1, 192, 7, 7]         221,184\n","     BatchNorm2d-200            [-1, 192, 7, 7]             384\n"," StochasticDepth-201            [-1, 192, 7, 7]               0\n","          MBConv-202            [-1, 192, 7, 7]               0\n","          Conv2d-203           [-1, 1152, 7, 7]         221,184\n","     BatchNorm2d-204           [-1, 1152, 7, 7]           2,304\n","            SiLU-205           [-1, 1152, 7, 7]               0\n","          Conv2d-206           [-1, 1152, 7, 7]          28,800\n","     BatchNorm2d-207           [-1, 1152, 7, 7]           2,304\n","            SiLU-208           [-1, 1152, 7, 7]               0\n","AdaptiveAvgPool2d-209           [-1, 1152, 1, 1]               0\n","          Conv2d-210             [-1, 48, 1, 1]          55,344\n","            SiLU-211             [-1, 48, 1, 1]               0\n","          Conv2d-212           [-1, 1152, 1, 1]          56,448\n","         Sigmoid-213           [-1, 1152, 1, 1]               0\n","SqueezeExcitation-214           [-1, 1152, 7, 7]               0\n","          Conv2d-215            [-1, 192, 7, 7]         221,184\n","     BatchNorm2d-216            [-1, 192, 7, 7]             384\n"," StochasticDepth-217            [-1, 192, 7, 7]               0\n","          MBConv-218            [-1, 192, 7, 7]               0\n","          Conv2d-219           [-1, 1152, 7, 7]         221,184\n","     BatchNorm2d-220           [-1, 1152, 7, 7]           2,304\n","            SiLU-221           [-1, 1152, 7, 7]               0\n","          Conv2d-222           [-1, 1152, 7, 7]          28,800\n","     BatchNorm2d-223           [-1, 1152, 7, 7]           2,304\n","            SiLU-224           [-1, 1152, 7, 7]               0\n","AdaptiveAvgPool2d-225           [-1, 1152, 1, 1]               0\n","          Conv2d-226             [-1, 48, 1, 1]          55,344\n","            SiLU-227             [-1, 48, 1, 1]               0\n","          Conv2d-228           [-1, 1152, 1, 1]          56,448\n","         Sigmoid-229           [-1, 1152, 1, 1]               0\n","SqueezeExcitation-230           [-1, 1152, 7, 7]               0\n","          Conv2d-231            [-1, 192, 7, 7]         221,184\n","     BatchNorm2d-232            [-1, 192, 7, 7]             384\n"," StochasticDepth-233            [-1, 192, 7, 7]               0\n","          MBConv-234            [-1, 192, 7, 7]               0\n","          Conv2d-235           [-1, 1152, 7, 7]         221,184\n","     BatchNorm2d-236           [-1, 1152, 7, 7]           2,304\n","            SiLU-237           [-1, 1152, 7, 7]               0\n","          Conv2d-238           [-1, 1152, 7, 7]          10,368\n","     BatchNorm2d-239           [-1, 1152, 7, 7]           2,304\n","            SiLU-240           [-1, 1152, 7, 7]               0\n","AdaptiveAvgPool2d-241           [-1, 1152, 1, 1]               0\n","          Conv2d-242             [-1, 48, 1, 1]          55,344\n","            SiLU-243             [-1, 48, 1, 1]               0\n","          Conv2d-244           [-1, 1152, 1, 1]          56,448\n","         Sigmoid-245           [-1, 1152, 1, 1]               0\n","SqueezeExcitation-246           [-1, 1152, 7, 7]               0\n","          Conv2d-247            [-1, 320, 7, 7]         368,640\n","     BatchNorm2d-248            [-1, 320, 7, 7]             640\n","          MBConv-249            [-1, 320, 7, 7]               0\n","          Conv2d-250           [-1, 1280, 7, 7]         409,600\n","     BatchNorm2d-251           [-1, 1280, 7, 7]           2,560\n","            SiLU-252           [-1, 1280, 7, 7]               0\n","AdaptiveAvgPool2d-253           [-1, 1280, 1, 1]               0\n","         Dropout-254                 [-1, 1280]               0\n","          Linear-255                    [-1, 2]           2,562\n","    EfficientNet-256                    [-1, 2]               0\n","         Softmax-257                    [-1, 2]               0\n","================================================================\n","Total params: 4,010,110\n","Trainable params: 4,010,110\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 173.64\n","Params size (MB): 15.30\n","Estimated Total Size (MB): 189.52\n","----------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["print(\"Hyperparameters Summary \\n\")\n","print(\"Learning Rate: \", lr)\n","print(\"Optmizer: ADAM\")\n","print(\"Loss Function: Binary Cross Entropy\")\n","print(\"N Epochs: \", num_epochs)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xNAlZNFyDNzD","executionInfo":{"status":"ok","timestamp":1687261482546,"user_tz":180,"elapsed":5,"user":{"displayName":"Gianni Shigeru Setoue Liveraro","userId":"06291562626291357927"}},"outputId":"a8f57276-eabe-4313-eb7a-a963a3ccdc05"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Hyperparameters Summary \n","\n","Learning Rate:  0.0001\n","Optmizer: ADAM\n","Loss Function: Binary Cross Entropy\n","N Epochs:  100\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2019-10-08T08:29:17.994063Z","start_time":"2019-10-08T08:04:07.796335Z"},"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"outputId":"f9eb03ce-9e16-4392-d71b-14a73f341384","id":"u9ai_PyMuIu5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3836 Acc: 0.9502 AUC: 0.9726\n","val Loss: 0.3288 Acc: 0.9884 AUC: 0.9844\n","Best model detected!\n","Epoch 1/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3330 Acc: 0.9825 AUC: 0.9902\n","val Loss: 0.3231 Acc: 0.9897 AUC: 0.9836\n","Best model detected!\n","Epoch 2/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3303 Acc: 0.9843 AUC: 0.9915\n","val Loss: 0.3210 Acc: 0.9949 AUC: 0.9905\n","Best model detected!\n","Epoch 3/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3229 Acc: 0.9917 AUC: 0.9943\n","val Loss: 0.3227 Acc: 0.9910 AUC: 0.9870\n","Epoch 4/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3240 Acc: 0.9897 AUC: 0.9931\n","val Loss: 0.3240 Acc: 0.9884 AUC: 0.9915\n","Epoch 5/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3219 Acc: 0.9915 AUC: 0.9946\n","val Loss: 0.3210 Acc: 0.9923 AUC: 0.9884\n","Best model detected!\n","Epoch 6/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3209 Acc: 0.9924 AUC: 0.9945\n","val Loss: 0.3202 Acc: 0.9923 AUC: 0.9863\n","Best model detected!\n","Epoch 7/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3204 Acc: 0.9935 AUC: 0.9929\n","val Loss: 0.3203 Acc: 0.9936 AUC: 0.9890\n","Epoch 8/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3200 Acc: 0.9937 AUC: 0.9923\n","val Loss: 0.3191 Acc: 0.9949 AUC: 0.9841\n","Best model detected!\n","Epoch 9/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3195 Acc: 0.9939 AUC: 0.9942\n","val Loss: 0.3181 Acc: 0.9961 AUC: 0.9843\n","Best model detected!\n","Epoch 10/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3202 Acc: 0.9931 AUC: 0.9940\n","val Loss: 0.3195 Acc: 0.9949 AUC: 0.9886\n","Epoch 11/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3187 Acc: 0.9946 AUC: 0.9955\n","val Loss: 0.3200 Acc: 0.9936 AUC: 0.9842\n","Epoch 12/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3178 Acc: 0.9951 AUC: 0.9945\n","val Loss: 0.3332 Acc: 0.9794 AUC: 0.9853\n","Epoch 13/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3200 Acc: 0.9930 AUC: 0.9925\n","val Loss: 0.3199 Acc: 0.9936 AUC: 0.9913\n","Epoch 14/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3180 Acc: 0.9951 AUC: 0.9958\n","val Loss: 0.3308 Acc: 0.9807 AUC: 0.9897\n","Epoch 15/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3200 Acc: 0.9928 AUC: 0.9932\n","val Loss: 0.3215 Acc: 0.9910 AUC: 0.9851\n","Epoch 16/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3184 Acc: 0.9949 AUC: 0.9940\n","val Loss: 0.3194 Acc: 0.9936 AUC: 0.9849\n","Epoch 17/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3193 Acc: 0.9937 AUC: 0.9925\n","val Loss: 0.3232 Acc: 0.9897 AUC: 0.9837\n","Epoch 18/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3191 Acc: 0.9937 AUC: 0.9914\n","val Loss: 0.3208 Acc: 0.9936 AUC: 0.9849\n","Epoch 19/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3177 Acc: 0.9957 AUC: 0.9926\n","val Loss: 0.3195 Acc: 0.9949 AUC: 0.9847\n","Epoch 20/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3173 Acc: 0.9962 AUC: 0.9940\n","val Loss: 0.3197 Acc: 0.9936 AUC: 0.9879\n","Epoch 21/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3169 Acc: 0.9964 AUC: 0.9959\n","val Loss: 0.3197 Acc: 0.9936 AUC: 0.9895\n","Epoch 22/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3175 Acc: 0.9955 AUC: 0.9956\n","val Loss: 0.3222 Acc: 0.9910 AUC: 0.9913\n","Epoch 23/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3173 Acc: 0.9958 AUC: 0.9959\n","val Loss: 0.3214 Acc: 0.9923 AUC: 0.9773\n","Epoch 24/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3195 Acc: 0.9937 AUC: 0.9944\n","val Loss: 0.3236 Acc: 0.9897 AUC: 0.9728\n","Epoch 25/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3163 Acc: 0.9971 AUC: 0.9967\n","val Loss: 0.3185 Acc: 0.9949 AUC: 0.9929\n","Epoch 26/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3186 Acc: 0.9946 AUC: 0.9941\n","val Loss: 0.3193 Acc: 0.9949 AUC: 0.9870\n","Epoch 27/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3174 Acc: 0.9957 AUC: 0.9949\n","val Loss: 0.3199 Acc: 0.9936 AUC: 0.9886\n","Epoch 28/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3185 Acc: 0.9946 AUC: 0.9924\n","val Loss: 0.3190 Acc: 0.9936 AUC: 0.9853\n","Epoch 29/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3185 Acc: 0.9949 AUC: 0.9923\n","val Loss: 0.3190 Acc: 0.9936 AUC: 0.9857\n","Epoch 30/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3178 Acc: 0.9953 AUC: 0.9934\n","val Loss: 0.3191 Acc: 0.9936 AUC: 0.9847\n","Epoch 31/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3186 Acc: 0.9946 AUC: 0.9948\n","val Loss: 0.3206 Acc: 0.9923 AUC: 0.9829\n","Epoch 32/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3179 Acc: 0.9955 AUC: 0.9937\n","val Loss: 0.3194 Acc: 0.9949 AUC: 0.9877\n","Epoch 33/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3170 Acc: 0.9962 AUC: 0.9940\n","val Loss: 0.3198 Acc: 0.9936 AUC: 0.9895\n","Epoch 34/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3166 Acc: 0.9968 AUC: 0.9957\n","val Loss: 0.3259 Acc: 0.9871 AUC: 0.9874\n","Epoch 35/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3188 Acc: 0.9946 AUC: 0.9947\n","val Loss: 0.3328 Acc: 0.9794 AUC: 0.9906\n","Epoch 36/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3214 Acc: 0.9917 AUC: 0.9943\n","val Loss: 0.3213 Acc: 0.9923 AUC: 0.9925\n","Epoch 37/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3167 Acc: 0.9966 AUC: 0.9944\n","val Loss: 0.3217 Acc: 0.9910 AUC: 0.9881\n","Epoch 38/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3175 Acc: 0.9955 AUC: 0.9961\n","val Loss: 0.3185 Acc: 0.9949 AUC: 0.9883\n","Epoch 39/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3169 Acc: 0.9962 AUC: 0.9944\n","val Loss: 0.3222 Acc: 0.9910 AUC: 0.9865\n","Epoch 40/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3174 Acc: 0.9958 AUC: 0.9951\n","val Loss: 0.3175 Acc: 0.9961 AUC: 0.9898\n","Best model detected!\n","Epoch 41/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3173 Acc: 0.9958 AUC: 0.9949\n","val Loss: 0.3188 Acc: 0.9949 AUC: 0.9903\n","Epoch 42/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3164 Acc: 0.9971 AUC: 0.9963\n","val Loss: 0.3191 Acc: 0.9949 AUC: 0.9871\n","Epoch 43/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3174 Acc: 0.9957 AUC: 0.9952\n","val Loss: 0.3188 Acc: 0.9949 AUC: 0.9905\n","Epoch 44/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3172 Acc: 0.9960 AUC: 0.9947\n","val Loss: 0.3226 Acc: 0.9910 AUC: 0.9843\n","Epoch 45/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3170 Acc: 0.9964 AUC: 0.9948\n","val Loss: 0.3204 Acc: 0.9923 AUC: 0.9810\n","Epoch 46/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3251 Acc: 0.9877 AUC: 0.9929\n","val Loss: 0.3199 Acc: 0.9936 AUC: 0.9925\n","Epoch 47/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3239 Acc: 0.9892 AUC: 0.9911\n","val Loss: 0.3195 Acc: 0.9936 AUC: 0.9962\n","Epoch 48/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3194 Acc: 0.9939 AUC: 0.9945\n","val Loss: 0.3184 Acc: 0.9949 AUC: 0.9904\n","Epoch 49/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3186 Acc: 0.9946 AUC: 0.9956\n","val Loss: 0.3242 Acc: 0.9884 AUC: 0.9745\n","Epoch 50/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3184 Acc: 0.9951 AUC: 0.9937\n","val Loss: 0.3185 Acc: 0.9949 AUC: 0.9917\n","Epoch 51/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3168 Acc: 0.9966 AUC: 0.9972\n","val Loss: 0.3185 Acc: 0.9949 AUC: 0.9952\n","Epoch 52/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3171 Acc: 0.9962 AUC: 0.9975\n","val Loss: 0.3197 Acc: 0.9936 AUC: 0.9953\n","Epoch 53/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3166 Acc: 0.9966 AUC: 0.9953\n","val Loss: 0.3187 Acc: 0.9949 AUC: 0.9954\n","Epoch 54/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3169 Acc: 0.9964 AUC: 0.9947\n","val Loss: 0.3172 Acc: 0.9961 AUC: 0.9949\n","Best model detected!\n","Epoch 55/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3170 Acc: 0.9964 AUC: 0.9947\n","val Loss: 0.3198 Acc: 0.9936 AUC: 0.9897\n","Epoch 56/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3206 Acc: 0.9926 AUC: 0.9902\n","val Loss: 0.3172 Acc: 0.9961 AUC: 0.9949\n","Best model detected!\n","Epoch 57/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3181 Acc: 0.9951 AUC: 0.9927\n","val Loss: 0.3210 Acc: 0.9923 AUC: 0.9837\n","Epoch 58/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3178 Acc: 0.9955 AUC: 0.9941\n","val Loss: 0.3230 Acc: 0.9897 AUC: 0.9894\n","Epoch 59/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3168 Acc: 0.9964 AUC: 0.9935\n","val Loss: 0.3199 Acc: 0.9936 AUC: 0.9865\n","Epoch 60/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3168 Acc: 0.9964 AUC: 0.9934\n","val Loss: 0.3220 Acc: 0.9910 AUC: 0.9880\n","Epoch 61/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3171 Acc: 0.9958 AUC: 0.9949\n","val Loss: 0.3184 Acc: 0.9949 AUC: 0.9909\n","Epoch 62/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3167 Acc: 0.9964 AUC: 0.9948\n","val Loss: 0.3229 Acc: 0.9897 AUC: 0.9855\n","Epoch 63/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3166 Acc: 0.9968 AUC: 0.9927\n","val Loss: 0.3185 Acc: 0.9949 AUC: 0.9876\n","Epoch 64/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3173 Acc: 0.9958 AUC: 0.9925\n","val Loss: 0.3236 Acc: 0.9897 AUC: 0.9805\n","Epoch 65/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3166 Acc: 0.9968 AUC: 0.9937\n","val Loss: 0.3235 Acc: 0.9897 AUC: 0.9767\n","Epoch 66/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3174 Acc: 0.9958 AUC: 0.9939\n","val Loss: 0.3184 Acc: 0.9949 AUC: 0.9832\n","Epoch 67/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3162 Acc: 0.9973 AUC: 0.9952\n","val Loss: 0.3203 Acc: 0.9936 AUC: 0.9832\n","Epoch 68/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3178 Acc: 0.9955 AUC: 0.9925\n","val Loss: 0.3196 Acc: 0.9936 AUC: 0.9854\n","Epoch 69/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3179 Acc: 0.9953 AUC: 0.9923\n","val Loss: 0.3211 Acc: 0.9923 AUC: 0.9893\n","Epoch 70/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3157 Acc: 0.9977 AUC: 0.9948\n","val Loss: 0.3194 Acc: 0.9936 AUC: 0.9924\n","Epoch 71/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3164 Acc: 0.9969 AUC: 0.9921\n","val Loss: 0.3220 Acc: 0.9923 AUC: 0.9931\n","Epoch 72/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3171 Acc: 0.9960 AUC: 0.9955\n","val Loss: 0.3208 Acc: 0.9923 AUC: 0.9889\n","Epoch 73/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3160 Acc: 0.9973 AUC: 0.9956\n","val Loss: 0.3197 Acc: 0.9936 AUC: 0.9900\n","Epoch 74/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3160 Acc: 0.9973 AUC: 0.9955\n","val Loss: 0.3228 Acc: 0.9897 AUC: 0.9861\n","Epoch 75/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3162 Acc: 0.9971 AUC: 0.9938\n","val Loss: 0.3208 Acc: 0.9923 AUC: 0.9921\n","Epoch 76/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3158 Acc: 0.9975 AUC: 0.9969\n","val Loss: 0.3219 Acc: 0.9910 AUC: 0.9874\n","Epoch 77/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3166 Acc: 0.9966 AUC: 0.9956\n","val Loss: 0.3237 Acc: 0.9897 AUC: 0.9883\n","Epoch 78/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3168 Acc: 0.9964 AUC: 0.9942\n","val Loss: 0.3248 Acc: 0.9884 AUC: 0.9803\n","Epoch 79/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3207 Acc: 0.9922 AUC: 0.9945\n","val Loss: 0.3254 Acc: 0.9871 AUC: 0.9943\n","Epoch 80/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3165 Acc: 0.9968 AUC: 0.9957\n","val Loss: 0.3231 Acc: 0.9897 AUC: 0.9880\n","Epoch 81/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3159 Acc: 0.9975 AUC: 0.9967\n","val Loss: 0.3211 Acc: 0.9923 AUC: 0.9861\n","Epoch 82/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3158 Acc: 0.9975 AUC: 0.9969\n","val Loss: 0.3194 Acc: 0.9949 AUC: 0.9952\n","Epoch 83/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3156 Acc: 0.9977 AUC: 0.9961\n","val Loss: 0.3234 Acc: 0.9897 AUC: 0.9797\n","Epoch 84/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3177 Acc: 0.9955 AUC: 0.9955\n","val Loss: 0.3227 Acc: 0.9910 AUC: 0.9834\n","Epoch 85/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3176 Acc: 0.9955 AUC: 0.9949\n"," Steps: 347 out 347 train Loss: 0.3171 Acc: 0.9960 AUC: 0.9956\n","val Loss: 0.3276 Acc: 0.9859 AUC: 0.9793\n","Epoch 87/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3176 Acc: 0.9957 AUC: 0.9975\n","val Loss: 0.3216 Acc: 0.9910 AUC: 0.9894\n","Epoch 88/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3173 Acc: 0.9958 AUC: 0.9966\n","val Loss: 0.3223 Acc: 0.9910 AUC: 0.9937\n","Epoch 89/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3162 Acc: 0.9971 AUC: 0.9960\n","val Loss: 0.3209 Acc: 0.9923 AUC: 0.9941\n","Epoch 90/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3170 Acc: 0.9962 AUC: 0.9941\n","val Loss: 0.3218 Acc: 0.9923 AUC: 0.9892\n","Epoch 91/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3167 Acc: 0.9966 AUC: 0.9952\n","val Loss: 0.3198 Acc: 0.9923 AUC: 0.9883\n","Epoch 92/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3161 Acc: 0.9971 AUC: 0.9950\n","val Loss: 0.3238 Acc: 0.9897 AUC: 0.9833\n","Epoch 93/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3176 Acc: 0.9957 AUC: 0.9942\n","val Loss: 0.3274 Acc: 0.9859 AUC: 0.9810\n","Epoch 94/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3156 Acc: 0.9977 AUC: 0.9965\n","val Loss: 0.3215 Acc: 0.9910 AUC: 0.9899\n","Epoch 95/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3154 Acc: 0.9978 AUC: 0.9946\n","val Loss: 0.3238 Acc: 0.9897 AUC: 0.9871\n","Epoch 96/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3168 Acc: 0.9964 AUC: 0.9945\n","val Loss: 0.3249 Acc: 0.9884 AUC: 0.9838\n","Epoch 97/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3185 Acc: 0.9948 AUC: 0.9967\n","val Loss: 0.3235 Acc: 0.9897 AUC: 0.9860\n","Epoch 98/99\n","----------\n"," Steps: 347 out 347 train Loss: 0.3198 Acc: 0.9933 AUC: 0.9937\n","val Loss: 0.3209 Acc: 0.9910 AUC: 0.9902\n","Epoch 99/99\n","----------\n"," Steps: 277 out 347 "]}],"source":["model, LOSS_train, ACC_train, LOSS_val, ACC_val = train_model(model, criterion, optimizer_model,\n","                       num_epochs=num_epochs)\n","\n","# Saving model\n","PATH = subpath+'/Trained_DLModel.pt'\n","torch.save(model.state_dict(),PATH)"]},{"cell_type":"code","source":["# Plot curves:\n","Epochs = np.arange(0,num_epochs)\n","\n","# Loss Curves:\n","Variable_x_Epochs(Epochs, LOSS_train, LOSS_val, \"Loss\")\n","# Acc Curves:\n","Variable_x_Epochs(Epochs, ACC_train, ACC_val, \"Accuracy\")"],"metadata":{"id":"xsfgD60mBcaz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Plot das curvas de TPR e FPR em função do Decision Threshold:\n","\n","Predictions, classes = validate_model(model)\n","AnalysisPlots(classes, Predictions)"],"metadata":{"id":"znsufwUTuIu6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Saving a copy of the experiment notebook\n","import shutil\n","\n","source_file = '/content/gdrive/Shareddrives/IA901 - Projeto Final/Notebooks/02_TrainTissueClassification.ipynb'\n","\n","# Copy the file to the destination directory\n","shutil.copy2(source_file, subpath)"],"metadata":{"id":"bA039FZJXKQw"},"execution_count":null,"outputs":[]}]}